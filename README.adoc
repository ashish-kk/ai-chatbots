= Overview

A simple demo ChatBot application that provides both an LLM and a RAG (Retrieval-Augmented Generation) based Chatbot to interact with a locally running LLM (Large Language Model).

WARNING: Currently this is relatively unoptimized with certain inefficiencies regarding working with embeddings. This will be speeded up in future releases.

With relevant *embeddings* the RAG Chatbot will answer queries on Red Hat OpenShift AI and Red Hat Open Data Science (ODS) that the pure LLM-based Chatbot will struggle with.

image::.images/rag-bot-screen-01.png[]


== Running the Application

There are two ways to run the application: using the provided Dockerfile or running the application locally.

NOTE: Older or low-memory machines may have performance issues running the application. Ideally a relatively recent laptop, or machine, with at least 32GB of RAM locally.

=== Common Prerequisites - The Local Large Language Model (LLM) and Runtime (Ollama)

. Clone this repository into a directory in your environment where you build a Python virtual machine. For example, this can be your home directory, or create a new directory specifically for this project then cd into it.

. Install the LLM runtime Ollama from link:https://ollama.com/[here] or follow the instructions in the link:https://github.com/ollama/ollama?tab=readme-ov-file[installation guide].

. Start Ollama running on your local machine either via the Ollama App you downloaded or via the command line `ollama serve`.

. In a separate terminal, run the `mistral` LLM service on your local machine via `ollama run mistral`

NOTE: The first time this will download the model and may take a few minutes, `mistral` is approximately 4GB in size. 

=== Running the Application using Podman (or Docker)

This is the *recommended* way to run the application and simplifies the Python dependencies.

[source,sh]
----
podman run --rm \
   --name ragbot \
   -e OLLAMA_BASE_URL="http://host.docker.internal:11434" \
   -p 7001:7001 \
   -p 7002:7002 \
   -p 7003:7003 \
   tonykay/ai-rag-chatbot:0.6.6
----

. Browse to your Chatbot of Choice - or both

[Attributes]
|===
|Landing Page | URL 


| LLM Chatbot (no RAG)
| `http://localhost:7001`

| RAG Chatbot 
| `http://localhost:7002`

| Both Chatbots side-by-side
| `http://localhost:7003`
|===


=== Running the Application Locally via Python

This method requires Python 3.11 or higher and assumes you will use a virtual environment, or venv, called venv-chatbot to install the dependencies.

. Create a Python Virtual Environment (venv) `venv` for the Python dependencies
+

[source,sh]
----
python -m venv venv-chatbot
----
+

NOTE: This will create a new directory called `venv-chatbot` in your current working directory. `git` will ignore this directory by default as it is already added to your `.gitignore`.

. Activate your new `venv`:
+

[source,sh]
----
source venv-chatbot/bin/activate
----
+

NOTE: Your prompt should now show `(venv-chatbot)` to indicate the virtual environment is active. However, this may vary depending on your shell.

. Install the dependencies using the requirements file. First:
+
 
[source,sh]
----
python -m pip install -r requirements.txt
----
+

.Sample Output (Your output may differ and include a message about upgrading pip)
[source,texinfo]
----
<TRUNCATED>

asn1-0.5.1 pyasn1-modules-0.3.0 pydantic-2.6.3 pydantic-core-2.16.3 pyjwt-2.8.0 pymupdf-1.23.26 pypika-0.48.9 pyproject_hooks-1.0.0 python-dateutil-2.9.0.post0 python-dotenv-1.0.1 python-engineio-4.9.0 python-graphql-client-0.4.3 python-multipart-0.0.6 python-socketio-5.11.1 regex-2023.12.25 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 safetensors-0.4.2 scikit-learn-1.4.1.post1 scipy-1.12.0 sentence_transformers-2.5.1 simple-websocket-1.0.0 six-1.16.0 sniffio-1.3.1 starlette-0.32.0.post1 sympy-1.12 syncer-2.0.3 tenacity-8.2.3 threadpoolctl-3.3.0 tiktoken-0.6.0 tokenizers-0.15.2 tomli-2.0.1 torch-2.2.1 tqdm-4.66.2 transformers-4.38.2 typer-0.9.0 typing-extensions-4.10.0 typing-inspect-0.9.0 uptrace-1.22.0 urllib3-2.2.1 uvicorn-0.25.0 uvloop-0.19.0 watchfiles-0.20.0 websocket-client-1.7.0 websockets-12.0 wrapt-1.16.0 wsproto-1.2.0 yarl-1.9.4 zipp-3.17.0
----

. Run the application:
+

[source,sh]
----
./entrypoint.sh
----

+

.Sample Output
[source,texinfo]
----
<TRUNCATED>

2024-03-25 06:48:52 - Use pytorch device_name: mps
2024-03-25 06:48:54 - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
File path is /Users/tok/Dropbox/PARAL/Projects/redhat-rag-demo/ai-chatbots/chatbot-rag/rag-persist/record_manager_cache.sql
Indexing stats: {'num_added': 334, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}
2024-03-25 06:49:01 - Your app is available at http://localhost:7002
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
::1 - - [25/Mar/2024 06:49:51] "GET grv/ HTTP/1.1" 200 -
----

. Browse to `http://localhost:7003` to access the application. The right hand iFrame will show the RAG Chatbot and the left hand iFrame will show the LLM Chatbot.

NOTE: The LLM Chatbot is available at `http://localhost:7001` and the RAG Chatbot is available at `http://localhost:7002` if you prefer to use them separately. And that RAG Chatbot can take a few moments to start.

== Using the Application

The application is a simple web interface that allows you to ask questions about Red Hat OpenShift AI and Red Hat Open Data Science (ODS). The application will use the locally running LLM and RAG to answer your questions.

NOTE: The first time you ask a question it may take a few seconds to respond as the application will need to generate embeddings for the question and the documents in the database.

=== Sample Questions (examples)

. What is Red Hat ods
. What is rh ods?

== Development

TBD


== Architecture

* chainlit
* LangChain
* chromadb - Chroma Database Vector Store to store and retrieve document chunks
* Hugging Face `sentance_transformers` - embeddings
*
*
*

== Running with a bind mount (Work in Progress)

Podman and Docker differ,

Docker cmd:

Podman cmd: 

Add `--uidmap 1000:0:1 --uidmap 0:1:1000`
```
podman run --uidmap 1000:0:1 --uidmap 0:1:1000 --rm --name ragnar -e OLLAMA_BASE_URL="http://host.docker.internal:11434" -v $(pwd):/home/user/app -p 7861:7860 tonykay/ai-rag-chatbot:0.1.0
```



https://github.com/containers/podman/issues/2898#issuecomment-934295483

* 
